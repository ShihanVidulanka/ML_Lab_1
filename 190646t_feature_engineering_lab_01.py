# -*- coding: utf-8 -*-
"""190646T_Feature Engineering_Lab_01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TZQhEdHo37W1-ovLV2QxBw7MlcVNu-WN

## 01 Import used libraries
"""

# required and used Libraries
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score

from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

"""## 02 Used Functions for feature engineering

*   Get dataframes data removing unwanted labels and null data
"""

from scipy.linalg import dft
def getTrainTestdata(df, required_label, removing_labels):

  # removeing null values and not required labels
  df = df.drop(removing_labels, axis =1).dropna()

  return df, df.pop(required_label)

"""*   Random Forest Classifier used to create the model"""

def useRandommForesterModel(X_train, y_train, X_test):
  # Define model. Specify a number for random_state to ensure same results each run
  model = RandomForestClassifier(random_state=1)

  # Fit model
  model.fit(X_train, y_train)

  # predict values using model
  predict_val = model.predict(X_test)

  return predict_val

"""*   K-Nearest Neighbor used to create the model"""

def useKNN(X_train, y_train, X_test):
  # standardize the data set
  scaler = StandardScaler()
  scaler.fit(X_train)

  # scale the data set using new scale
  X_train_scaled = scaler.transform(X_train)
  X_test_scaled = scaler.transform(X_test)

  # create KNN model
  classifier = KNeighborsClassifier(n_neighbors=5)
  classifier.fit(X_train_scaled, y_train)
  y_pred = classifier.predict(X_test_scaled)
  return y_pred

"""* Get the accuracy and report of the predictions"""

def getSummary(y_test, predict_val):
  # get the mean absolute error of the model
  MAE = mean_absolute_error(y_test, predict_val)

  # get the accuracy of the model
  accuracy = accuracy_score(y_test, predict_val)
  report = classification_report(y_test, predict_val)
  return MAE, report,accuracy

"""*   select best K number of features using sklearn **SelectBest** function which uses **f_regression**"""

from sklearn.feature_selection import SelectKBest

from sklearn.feature_selection import f_regression

def getSelectedFeatures(k, X_train, y_train):
  # define number of features to keep "k"

  # perform feature selection
  X_new = SelectKBest(f_regression, k=k).fit_transform(X_train, y_train)

  # get feature names of selected features
  selected_features = X_train.columns[SelectKBest(f_regression, k=k).fit(X_train, y_train).get_support()]

  return selected_features

"""*   Add a ***Principal Component Analysis(PCA)*** and get additional features by finding principal components"""

from sklearn.decomposition import PCA
def apply_pca(n_omponents, X_train, X_test, X_test_predict, standardize=True):
    # Standardize
    if standardize:
        X_train = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)
    # Create principal components
    pca = PCA(n_omponents, svd_solver='full')
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)
    X_test_predict_pca = pca.transform(X_test_predict)
    # Convert to dataframe
    component_names = [f"PC{i+1}" for i in range(X_train_pca.shape[1])]
    X_train_pca = pd.DataFrame(X_train_pca, columns=component_names)
    X_test_pca = pd.DataFrame(X_test_pca, columns=component_names)
    X_test_predict_pca = pd.DataFrame(X_test_predict_pca, columns=component_names)
    # Create loadings
    loadings = pd.DataFrame(
        pca.components_.T,  # transpose the matrix of loadings
        columns=component_names,  # so the columns are the principal components
        index=X_train.columns,  # and the rows are the original features
    )
    return pca, X_train_pca, X_test_pca, X_test_predict_pca, loadings

"""## 03 Reducing features by feature engineering of Label 01

*   Get dataset and remove un wanted labels
"""

train = pd.read_csv("drive/MyDrive/ML_Lab1/Lab_1_train.csv")
test = pd.read_csv("drive/MyDrive/ML_Lab1/Lab_1_valid.csv")
test_predict = pd.read_csv("drive/MyDrive/ML_Lab1/test.csv")


X_train_label1, y_train_label1= getTrainTestdata(train, "label_1",['label_2','label_3','label_4'])
X_test_label1, y_test_label1  = getTrainTestdata(test, "label_1",['label_2','label_3','label_4'])
X_test_predict_label1  = test_predict.drop(['label_1','label_2','label_3','label_4'], axis =1).dropna()
X_train_label1.head()

"""* create a model using Random Forest before feature engineering"""

from sklearn.metrics import classification_report, confusion_matrix

predict_values = useRandommForesterModel(X_train_label1, y_train_label1, X_test_label1)
MAE, classification_report, accuracy = getSummary(y_test_label1, predict_values)
print("Mean absolute error before feature engineering Lable_1: ", MAE)
print("Classification report: \n", classification_report)

"""* create a model using Random KNN before feature engineering"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_label1, y_train_label1, X_test_label1)
MAE, classification_report, accuracy = getSummary(y_test_label1, y_pred)

print(classification_report)

"""* Add KNN predicted data since it has the better accuracy to csv file"""

predicted_values = dict()

Before_FE_predict_label = useKNN(X_train_label1, y_train_label1, X_test_predict_label1)
predicted_values['Predicted labels before feature engineering'] = Before_FE_predict_label

"""* Find best k features and select

    (before selecting features checked if k is enough to achieve a better accuracy and finally come up with k features is enough)
"""

selected_features = getSelectedFeatures(60, X_train_label1, y_train_label1)

# print selected features
print(selected_features)

"""* Check the accuracy of the selected features are enogh or not using random forester"""

from sklearn.metrics import classification_report, confusion_matrix
X_train_selected_label1 = X_train_label1[list(selected_features)]
X_test_selected_label1 = X_test_label1[list(selected_features)]
X_test_predict_selected_label1 = X_test_predict_label1[list(selected_features)]

predict_values = useRandommForesterModel(X_train_selected_label1, y_train_label1, X_test_selected_label1)
MAE, classification_report, accuracy = getSummary(y_test_label1, predict_values)
print("Mean absolute error after reducing features of Lable_1: ", MAE)
print("Classification report: \n", classification_report)

"""* Check the accuracy of the selected features are enogh or not using KNN"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_selected_label1, y_train_label1, X_test_selected_label1)
MAE, classification_report, accuracy = getSummary(y_test_label1, y_pred)

print("using KNN after redusing the features to create model: ")
print(classification_report)

"""* Create a PCA to create new features using train data"""

pca, X_train_pca, X_test_pca, X_test_predict_pca, loadings = apply_pca(5, X_train_label1, X_test_label1, X_test_predict_label1)
# print(X_test_pca.keys())
print(loadings.to_string())

"""* Join the newly created features with the selected features and find the status of the model using Random forest"""

from sklearn.metrics import classification_report, confusion_matrix

X_train_new_label1 = X_train_selected_label1.join(X_train_pca)
X_test_new_label1 = X_test_selected_label1.join(X_test_pca)
X_test_predict_new_label1 = X_test_predict_selected_label1.join(X_test_predict_pca)

predict_values = useRandommForesterModel(X_train_new_label1, y_train_label1, X_test_new_label1)
MAE, classification_report, accuracy = getSummary(y_test_label1, predict_values)

print("Mean absolute error after adding new pca features of the model Lable_1: ", MAE)
print("Classification report: \n", classification_report)

"""* Find the status of the model using KNN after joining PCAs"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_new_label1, y_train_label1, X_test_new_label1)
MAE, classification_report, accuracy = getSummary(y_test_label1, y_pred)

print(classification_report)

"""* Add KNN predicted data since it has the better accuracy to csv file"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_new_label1, y_train_label1, X_test_predict_new_label1)
print(predict_values)
predicted_values['Predicted labels after feature engineering'] = y_pred
predicted_values['No of new features'] = len(X_test_predict_new_label1.keys())
column_names = ['new_feature_'+str(x-2) for x in range(len(X_test_predict_new_label1.keys())+3)]
column_names[0] = 'Predicted labels before feature engineering'
column_names[1] = 'Predicted labels after feature engineering'
column_names[2] = 'No of new features'
# new_features = pd.Dataframe()

# Create DataFrame
df = pd.DataFrame(predicted_values)
df = df.join(X_test_predict_new_label1)
df.columns = column_names
df.head()

df.to_csv("drive/MyDrive/ML_Lab1/Lab_1_Label_1.csv", index=False)

"""## 04 Reducing features by feature engineering of Label 02

*   Get dataset and remove un wanted labels
"""

train = pd.read_csv("drive/MyDrive/ML_Lab1/Lab_1_train.csv")
test = pd.read_csv("drive/MyDrive/ML_Lab1/Lab_1_valid.csv")
test_predict = pd.read_csv("drive/MyDrive/ML_Lab1/test.csv")


X_train_label2, y_train_label2= getTrainTestdata(train, "label_2",['label_1','label_3','label_4'])
X_test_label2, y_test_label2  = getTrainTestdata(test, "label_2",['label_1','label_3','label_4'])
X_test_predict_label2  = test_predict.drop(['label_1','label_2','label_3','label_4'], axis =1).dropna()
X_train_label2.head()

"""* create a model using Random Forest before feature engineering"""

from sklearn.metrics import classification_report, confusion_matrix

predict_values = useRandommForesterModel(X_train_label2, y_train_label2, X_test_label2)
MAE, classification_report, accuracy = getSummary(y_test_label2, predict_values)
print("Mean absolute error before feature engineering Lable_2: ", MAE)
print("Classification report: \n", classification_report)

"""* create a model using Random KNN before feature engineering"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_label2, y_train_label2, X_test_label2)
MAE, classification_report, accuracy = getSummary(y_test_label2, y_pred)

print(classification_report)

"""* Add KNN predicted data since it has the better accuracy to csv file"""

predicted_values = dict()

Before_FE_predict_label = useKNN(X_train_label2, y_train_label2, X_test_predict_label2)
predicted_values['Predicted labels before feature engineering'] = Before_FE_predict_label

"""* Find best k features and select

    (before selecting features checked if k is enough to achieve a better accuracy and finally come up with k features is enough)
"""

selected_features = getSelectedFeatures(60, X_train_label2, y_train_label2)

# print selected features
print(selected_features)

"""* Check the accuracy of the selected features are enogh or not using random forester"""

from sklearn.metrics import classification_report, confusion_matrix
X_train_selected_label2 = X_train_label2[list(selected_features)]
X_test_selected_label2 = X_test_label2[list(selected_features)]
X_test_predict_selected_label2 = X_test_predict_label2[list(selected_features)]

predict_values = useRandommForesterModel(X_train_selected_label2, y_train_label2, X_test_selected_label2)
MAE, classification_report, accuracy = getSummary(y_test_label2, predict_values)
print("Mean absolute error after reducing features of Lable_2: ", MAE)
print("Classification report: \n", classification_report)

"""* Check the accuracy of the selected features are enogh or not using KNN"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_selected_label2, y_train_label2, X_test_selected_label2)
MAE, classification_report, accuracy = getSummary(y_test_label2, y_pred)

print("using KNN after redusing the features to create model: ")
print(classification_report)

"""* Add KNN predicted data since it has the better accuracy to csv file
    
    (do not added pca since the test data will be reduced due to increasing the NaN data of PCAs)
"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_selected_label2, y_train_label2, X_test_predict_selected_label2)
print(predict_values)
predicted_values['Predicted labels after feature engineering'] = y_pred
predicted_values['No of new features'] = len(X_test_predict_selected_label2.keys())
column_names = ['new_feature_'+str(x-2) for x in range(len(X_test_predict_selected_label2.keys())+3)]
column_names[0] = 'Predicted labels before feature engineering'
column_names[1] = 'Predicted labels after feature engineering'
column_names[2] = 'No of new features'
# new_features = pd.Dataframe()

# Create DataFrame
df = pd.DataFrame(predicted_values)
df = df.join(X_test_predict_selected_label2)
df.columns = column_names
df.head()

df.to_csv("drive/MyDrive/ML_Lab1/Lab_1_Label_2.csv", index=False)

"""## 05 Reducing features by feature engineering of Label 03

*   Get dataset and remove un wanted labels
"""

train = pd.read_csv("drive/MyDrive/ML_Lab1/Lab_1_train.csv")
test = pd.read_csv("drive/MyDrive/ML_Lab1/Lab_1_valid.csv")
test_predict = pd.read_csv("drive/MyDrive/ML_Lab1/test.csv")


X_train_label3, y_train_label3= getTrainTestdata(train, "label_3",['label_1','label_2','label_4'])
X_test_label3, y_test_label3  = getTrainTestdata(test, "label_3",['label_1','label_2','label_4'])
X_test_predict_label3  = test_predict.drop(['label_1','label_2','label_3','label_4'], axis =1).dropna()
X_train_label3.head()

"""* create a model using Random Forest before feature engineering"""

from sklearn.metrics import classification_report, confusion_matrix

predict_values = useRandommForesterModel(X_train_label3, y_train_label3, X_test_label3)
MAE, classification_report, accuracy = getSummary(y_test_label3, predict_values)
print("Mean absolute error before feature engineering Lable_3: ", MAE)
print("Classification report: \n", classification_report)

"""* create a model using Random KNN before feature engineering"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_label3, y_train_label3, X_test_label3)
MAE, classification_report, accuracy = getSummary(y_test_label3, y_pred)

print(classification_report)

"""* Add KNN predicted data since it has the better accuracy to csv file"""

predicted_values = dict()

Before_FE_predict_label = useKNN(X_train_label3, y_train_label3, X_test_predict_label3)
predicted_values['Predicted labels before feature engineering'] = Before_FE_predict_label

"""* Find best k features and select

    (before selecting features checked if k is enough to achieve a better accuracy and finally come up with k features is enough)
"""

selected_features = getSelectedFeatures(4, X_train_label3, y_train_label3)

# print selected features
print(selected_features)

"""* Check the accuracy of the selected features are enogh or not using random forester"""

from sklearn.metrics import classification_report, confusion_matrix
X_train_selected_label3 = X_train_label3[list(selected_features)]
X_test_selected_label3 = X_test_label3[list(selected_features)]
X_test_predict_selected_label3 = X_test_predict_label3[list(selected_features)]

predict_values = useRandommForesterModel(X_train_selected_label3, y_train_label3, X_test_selected_label3)
MAE, classification_report, accuracy = getSummary(y_test_label3, predict_values)
print("Mean absolute error after reducing features of Lable_3: ", MAE)
print("Classification report: \n", classification_report)

"""* Check the accuracy of the selected features are enogh or not using KNN"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_selected_label3, y_train_label3, X_test_selected_label3)
MAE, classification_report, accuracy = getSummary(y_test_label3, y_pred)

print("using KNN after redusing the features to create model: ")
print(classification_report)

"""* Create a PCA to create new features using train data"""

pca, X_train_pca, X_test_pca, X_test_predict_pca, loadings = apply_pca(6, X_train_label3, X_test_label3, X_test_predict_label3)
# print(X_test_pca.keys())
print(loadings.to_string())

"""* Join the newly created features with the selected features and find the status of the model using Random forest"""

from sklearn.metrics import classification_report, confusion_matrix

X_train_new_label3 = X_train_selected_label3.join(X_train_pca)
X_test_new_label3 = X_test_selected_label3.join(X_test_pca)
X_test_predict_new_label3 = X_test_predict_selected_label3.join(X_test_predict_pca)

predict_values = useRandommForesterModel(X_train_new_label3, y_train_label3, X_test_new_label3)
MAE, classification_report, accuracy = getSummary(y_test_label3, predict_values)

print("Mean absolute error after adding new pca features of the model Lable_3: ", MAE)
print("Classification report: \n", classification_report)

"""* Find the status of the model using KNN after joining PCAs"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_new_label3, y_train_label3, X_test_new_label3)
MAE, classification_report, accuracy = getSummary(y_test_label3, y_pred)

print(classification_report)

"""* Add KNN predicted data since it has the better accuracy to csv file"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_new_label3, y_train_label3, X_test_predict_new_label3)
print(predict_values)
predicted_values['Predicted labels after feature engineering'] = y_pred
predicted_values['No of new features'] = len(X_test_predict_new_label3.keys())
column_names = ['new_feature_'+str(x-2) for x in range(len(X_test_predict_new_label3.keys())+3)]
column_names[0] = 'Predicted labels before feature engineering'
column_names[1] = 'Predicted labels after feature engineering'
column_names[2] = 'No of new features'
# new_features = pd.Dataframe()

# Create DataFrame
df = pd.DataFrame(predicted_values)
df = df.join(X_test_predict_new_label3)
df.columns = column_names
df.head()

df.to_csv("drive/MyDrive/ML_Lab1/Lab_1_Label_3.csv", index=False)

"""## 06 Reducing features by feature engineering of Label 04

*   Get dataset and remove un wanted labels
"""

train = pd.read_csv("drive/MyDrive/ML_Lab1/Lab_1_train.csv")
test = pd.read_csv("drive/MyDrive/ML_Lab1/Lab_1_valid.csv")
test_predict = pd.read_csv("drive/MyDrive/ML_Lab1/test.csv")


X_train_label4, y_train_label4= getTrainTestdata(train, "label_4",['label_1','label_2','label_3'])
X_test_label4, y_test_label4  = getTrainTestdata(test, "label_4",['label_1','label_2','label_3'])
X_test_predict_label4  = test_predict.drop(['label_1','label_2','label_3','label_4'], axis =1).dropna()
X_train_label4.head()

"""* create a model using Random Forest before feature engineering"""

from sklearn.metrics import classification_report, confusion_matrix

predict_values = useRandommForesterModel(X_train_label4, y_train_label4, X_test_label4)
MAE, classification_report, accuracy = getSummary(y_test_label4, predict_values)
print("Mean absolute error before feature engineering Lable_4: ", MAE)
print("Classification report: \n", classification_report)

"""* create a model using Random KNN before feature engineering"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_label4, y_train_label4, X_test_label4)
MAE, classification_report, accuracy = getSummary(y_test_label4, y_pred)

print(classification_report)

"""* Add KNN predicted data since it has the better accuracy to csv file"""

predicted_values = dict()

Before_FE_predict_label = useKNN(X_train_label4, y_train_label4, X_test_predict_label4)
predicted_values['Predicted labels before feature engineering'] = Before_FE_predict_label

"""* Find best k features and select

    (before selecting features checked if k is enough to achieve a better accuracy and finally come up with k features is enough)
"""

selected_features = getSelectedFeatures(40, X_train_label4, y_train_label4)

# print selected features
print(selected_features)

"""* Check the accuracy of the selected features are enogh or not using random forester"""

from sklearn.metrics import classification_report, confusion_matrix
X_train_selected_label4 = X_train_label4[list(selected_features)]
X_test_selected_label4 = X_test_label4[list(selected_features)]
X_test_predict_selected_label4 = X_test_predict_label4[list(selected_features)]

predict_values = useRandommForesterModel(X_train_selected_label4, y_train_label4, X_test_selected_label4)
MAE, classification_report, accuracy = getSummary(y_test_label4, predict_values)
print("Mean absolute error after reducing features of Lable_4: ", MAE)
print("Classification report: \n", classification_report)

"""* Check the accuracy of the selected features are enogh or not using KNN"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_selected_label4, y_train_label4, X_test_selected_label4)
MAE, classification_report, accuracy = getSummary(y_test_label4, y_pred)

print("using KNN after redusing the features to create model: ")
print(classification_report)

"""* Create a PCA to create new features using train data"""

pca, X_train_pca, X_test_pca, X_test_predict_pca, loadings = apply_pca(10, X_train_label4, X_test_label4, X_test_predict_label4)
# print(X_test_pca.keys())
print(loadings.to_string())

"""* Join the newly created features with the selected features and find the status of the model using Random forest"""

from sklearn.metrics import classification_report, confusion_matrix

X_train_new_label4 = X_train_selected_label4.join(X_train_pca)
X_test_new_label4 = X_test_selected_label4.join(X_test_pca)
X_test_predict_new_label4 = X_test_predict_selected_label4.join(X_test_predict_pca)

predict_values = useRandommForesterModel(X_train_new_label4, y_train_label4, X_test_new_label4)
MAE, classification_report, accuracy = getSummary(y_test_label4, predict_values)

print("Mean absolute error after adding new pca features of the model Lable_4: ", MAE)
print("Classification report: \n", classification_report)

"""* Find the status of the model using KNN after joining PCAs"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_new_label4, y_train_label4, X_test_new_label4)
MAE, classification_report, accuracy = getSummary(y_test_label4, y_pred)

print(classification_report)

"""* Add KNN predicted data since it has the better accuracy to csv file"""

from sklearn.metrics import classification_report, confusion_matrix

y_pred = useKNN(X_train_new_label4, y_train_label4, X_test_predict_new_label4)
print(predict_values)
predicted_values['Predicted labels after feature engineering'] = y_pred
predicted_values['No of new features'] = len(X_test_predict_new_label4.keys())
column_names = ['new_feature_'+str(x-2) for x in range(len(X_test_predict_new_label4.keys())+3)]
column_names[0] = 'Predicted labels before feature engineering'
column_names[1] = 'Predicted labels after feature engineering'
column_names[2] = 'No of new features'
# new_features = pd.Dataframe()

# Create DataFrame
df = pd.DataFrame(predicted_values)
df = df.join(X_test_predict_new_label4)
df.columns = column_names
df.head()

df.to_csv("drive/MyDrive/ML_Lab1/Lab_1_Label_4.csv", index=False)